{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for training ML models on dataset for Open-Source Reservoir (CMG simulations)\n",
    "\n",
    "**Important:** running this notebook takes a long time (in the order of 24 hours). The idea here is to run cross-validation implemented in this notebooks once to find best hyperparameters and then we can use a different notebook to train production models which will take much less time. \n",
    "\n",
    "-------\n",
    "Code in this notebook trains multiple models and performs k-fold cross-validation. \n",
    "Exploration of model/training parameters includes:\n",
    "* varying degree of interpolating polygons for studied timeseries\n",
    "* varying the numbers of neurons in the NN hidden layers (networks with two hidden layers are studied)\n",
    "* varying the amount of training (number of epochs)\n",
    "\n",
    "MAPE and MAE error measures (presented in the error dataframe) are averaged across both validation cases and k-folds.  \n",
    "\n",
    "This exporation is repeated for all quantities of interest (pressures and temperatures for producer wells).\n",
    "\n",
    "Handing of the analysis results is implemented in such a way where old results are red in and new results are added to the growing dataframe; \n",
    "after each interation, the up-to-date dataframe is saved to disk to prevent loss of results in situations where jupyter sessions are interrupted.\n",
    "\n",
    "-------\n",
    "\n",
    "*Written by: Dmitry Duplyakin (dmitry.duplyakin@nrel.gov) in collaboration with the National Renewable Energy Laboratories.*\n",
    "\n",
    "*Full team: Dmitry Duplyakin, Koenraad F. Beckers, Drew L. Siler, Michael J. Martin, Henry E. Johnston*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "loglevel = 'WARNING'\n",
    "logging.basicConfig(level=os.environ.get(\"LOGLEVEL\", loglevel))\n",
    "\n",
    "# Import config file that is specific to CMG dataset\n",
    "sys.path.append('../data/OpenSourceReservoir-CMG')\n",
    "sys.path.append('../')\n",
    "\n",
    "import config_cmg as config\n",
    "\n",
    "#sys.path.append('..')\n",
    "from reservoir.reservoir import Reservoir, ReservoirPredictionEnsemble\n",
    "\n",
    "from polynomial import get_polynomial_func\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data (from Xcel files to Pandas dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: case 0, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00000_ML_BHS_JA.xlsx\n",
      "Processing: case 1, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00001_ML_BHS_JA.xlsx\n",
      "Processing: case 2, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00002_ML_BHS_JA.xlsx\n",
      "Processing: case 3, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00003_ML_BHS_JA.xlsx\n",
      "Processing: case 4, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00004_ML_BHS_JA.xlsx\n",
      "Processing: case 5, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00005_ML_BHS_JA.xlsx\n",
      "Processing: case 6, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00006_ML_BHS_JA.xlsx\n",
      "Processing: case 7, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00007_ML_BHS_JA.xlsx\n",
      "Processing: case 8, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00008_ML_BHS_JA.xlsx\n",
      "Processing: case 9, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00009_ML_BHS_JA.xlsx\n",
      "Processing: case 10, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00010_ML_BHS_JA.xlsx\n",
      "Processing: case 11, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00011_ML_BHS_JA.xlsx\n",
      "Processing: case 12, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00012_ML_BHS_JA.xlsx\n",
      "Processing: case 13, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00013_ML_BHS_JA.xlsx\n",
      "Processing: case 14, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00014_ML_BHS_JA.xlsx\n",
      "Processing: case 15, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00015_ML_BHS_JA.xlsx\n",
      "Processing: case 16, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00016_ML_BHS_JA.xlsx\n",
      "Processing: case 17, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00017_ML_BHS_JA.xlsx\n",
      "Processing: case 18, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00018_ML_BHS_JA.xlsx\n",
      "Processing: case 19, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00019_ML_BHS_JA.xlsx\n",
      "Processing: case 20, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00020_ML_BHS_JA.xlsx\n",
      "Processing: case 21, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00021_ML_BHS_JA.xlsx\n",
      "Processing: case 22, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00022_ML_BHS_JA.xlsx\n",
      "Processing: case 23, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00023_ML_BHS_JA.xlsx\n",
      "Processing: case 24, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00024_ML_BHS_JA.xlsx\n",
      "Processing: case 25, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00025_ML_BHS_JA.xlsx\n",
      "Processing: case 26, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00026_ML_BHS_JA.xlsx\n",
      "Processing: case 27, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00027_ML_BHS_JA.xlsx\n",
      "Processing: case 28, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00028_ML_BHS_JA.xlsx\n",
      "Processing: case 29, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00029_ML_BHS_JA.xlsx\n",
      "Processing: case 30, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00030_ML_BHS_JA.xlsx\n",
      "Processing: case 31, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00031_ML_BHS_JA.xlsx\n",
      "Processing: case 32, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00032_ML_BHS_JA.xlsx\n",
      "Processing: case 33, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00033_ML_BHS_JA.xlsx\n",
      "Processing: case 34, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00034_ML_BHS_JA.xlsx\n",
      "Processing: case 35, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00035_ML_BHS_JA.xlsx\n",
      "Processing: case 36, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00036_ML_BHS_JA.xlsx\n",
      "Processing: case 37, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00037_ML_BHS_JA.xlsx\n",
      "Processing: case 38, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00038_ML_BHS_JA.xlsx\n",
      "Processing: case 39, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00039_ML_BHS_JA.xlsx\n",
      "Processing: case 40, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00040_ML_BHS_JA.xlsx\n",
      "Processing: case 41, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00041_ML_BHS_JA.xlsx\n",
      "Processing: case 42, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00042_ML_BHS_JA.xlsx\n",
      "Processing: case 43, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00043_ML_BHS_JA.xlsx\n",
      "Processing: case 44, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00044_ML_BHS_JA.xlsx\n",
      "Processing: case 45, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00045_ML_BHS_JA.xlsx\n",
      "Processing: case 46, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00046_ML_BHS_JA.xlsx\n",
      "Processing: case 47, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00047_ML_BHS_JA.xlsx\n",
      "Processing: case 48, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00048_ML_BHS_JA.xlsx\n",
      "Processing: case 49, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00049_ML_BHS_JA.xlsx\n",
      "Processing: case 50, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00050_ML_BHS_JA.xlsx\n",
      "Processing: case 51, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00051_ML_BHS_JA.xlsx\n",
      "Processing: case 52, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00052_ML_BHS_JA.xlsx\n",
      "Processing: case 53, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00053_ML_BHS_JA.xlsx\n",
      "Processing: case 54, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00054_ML_BHS_JA.xlsx\n",
      "Processing: case 55, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00055_ML_BHS_JA.xlsx\n",
      "Processing: case 56, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00056_ML_BHS_JA.xlsx\n",
      "Processing: case 57, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00057_ML_BHS_JA.xlsx\n",
      "Processing: case 58, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00058_ML_BHS_JA.xlsx\n",
      "Processing: case 59, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00059_ML_BHS_JA.xlsx\n",
      "Processing: case 60, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00060_ML_BHS_JA.xlsx\n",
      "Processing: case 61, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00061_ML_BHS_JA.xlsx\n",
      "Processing: case 62, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00062_ML_BHS_JA.xlsx\n",
      "Processing: case 63, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00063_ML_BHS_JA.xlsx\n",
      "Processing: case 64, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00064_ML_BHS_JA.xlsx\n",
      "Processing: case 65, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00065_ML_BHS_JA.xlsx\n",
      "Processing: case 66, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00066_ML_BHS_JA.xlsx\n",
      "Processing: case 67, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00067_ML_BHS_JA.xlsx\n",
      "Processing: case 68, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00068_ML_BHS_JA.xlsx\n",
      "Processing: case 69, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00069_ML_BHS_JA.xlsx\n",
      "Processing: case 70, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00070_ML_BHS_JA.xlsx\n",
      "Processing: case 71, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00071_ML_BHS_JA.xlsx\n",
      "Processing: case 72, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00072_ML_BHS_JA.xlsx\n",
      "Processing: case 73, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00073_ML_BHS_JA.xlsx\n",
      "Processing: case 74, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00074_ML_BHS_JA.xlsx\n",
      "Processing: case 75, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00075_ML_BHS_JA.xlsx\n",
      "Processing: case 76, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00076_ML_BHS_JA.xlsx\n",
      "Processing: case 77, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00077_ML_BHS_JA.xlsx\n",
      "Processing: case 78, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00078_ML_BHS_JA.xlsx\n",
      "Processing: case 79, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00079_ML_BHS_JA.xlsx\n",
      "Processing: case 80, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00080_ML_BHS_JA.xlsx\n",
      "Processing: case 81, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00081_ML_BHS_JA.xlsx\n",
      "Processing: case 82, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00082_ML_BHS_JA.xlsx\n",
      "Processing: case 83, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00083_ML_BHS_JA.xlsx\n",
      "Processing: case 84, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00084_ML_BHS_JA.xlsx\n",
      "Processing: case 85, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00085_ML_BHS_JA.xlsx\n",
      "Processing: case 86, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00086_ML_BHS_JA.xlsx\n",
      "Processing: case 87, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00087_ML_BHS_JA.xlsx\n",
      "Processing: case 88, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00088_ML_BHS_JA.xlsx\n",
      "Processing: case 89, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00089_ML_BHS_JA.xlsx\n",
      "Processing: case 90, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00090_ML_BHS_JA.xlsx\n",
      "Processing: case 91, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00091_ML_BHS_JA.xlsx\n",
      "Processing: case 92, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00092_ML_BHS_JA.xlsx\n",
      "Processing: case 93, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00093_ML_BHS_JA.xlsx\n",
      "Processing: case 94, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00094_ML_BHS_JA.xlsx\n",
      "Processing: case 95, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00095_ML_BHS_JA.xlsx\n",
      "Processing: case 96, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00096_ML_BHS_JA.xlsx\n",
      "Processing: case 97, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00097_ML_BHS_JA.xlsx\n",
      "Processing: case 98, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00098_ML_BHS_JA.xlsx\n",
      "Processing: case 99, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00099_ML_BHS_JA.xlsx\n",
      "Processing: case 100, file ../data/OpenSourceReservoir-CMG/ML_BHS_JA_00100_ML_BHS_JA.xlsx\n"
     ]
    }
   ],
   "source": [
    "cases_list = []\n",
    "\n",
    "data_dir = \"../data/OpenSourceReservoir-CMG/\"\n",
    "filename_pattern = \"ML_BHS_JA_*_ML_BHS_JA.xlsx\"\n",
    "\n",
    "# This assumes file names like: ../data/TETRADG-60_cases/TETRADG_v2_<case #>.xlsx\n",
    "cases = dict([(int(f.split(\"/\")[-1].replace(\".xlsx\",\"\").replace(\"ML_BHS_JA\", \"\").replace(\"_\",\"\")),f) for f \n",
    "         in glob.glob(data_dir + filename_pattern)])\n",
    "\n",
    "for case in sorted(cases):\n",
    "\n",
    "    file = cases[case]\n",
    "    print(\"Processing: case %d, file %s\" % (case, file))\n",
    "    \n",
    "    config_for_case = config\n",
    "    \n",
    "    # Override some config setting for this particular vis/analysis\n",
    "    config_for_case.flow_unit = \"kg/day\"\n",
    "    config_for_case.timeseries_file = file\n",
    "\n",
    "    cases_list.append(Reservoir(config_for_case, energy_calc=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine different scenarios into an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = ReservoirPredictionEnsemble(config, cases_list)    \n",
    "ens.scale()\n",
    "\n",
    "# To see individual scaled timeseries, do the following:\n",
    "# ens[0].scaled_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation (with cross-validation)\n",
    "\n",
    "Keep in mind that running the following code takes a **long time!** (unless `quick_test` is set to `True`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 53 54 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75\n",
      " 76 77 79 80 81 84 86 87 88 89 90 91 92 93 96 97 98 99], test: [ 17  36  52  55  78  82  83  85  94  95 100]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>im1</th>\n",
       "      <th>im2</th>\n",
       "      <th>im3</th>\n",
       "      <th>im4</th>\n",
       "      <th>pm1</th>\n",
       "      <th>pm2</th>\n",
       "      <th>pm3</th>\n",
       "      <th>pm4</th>\n",
       "      <th>pm5</th>\n",
       "      <th>pm6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.43091</td>\n",
       "      <td>0.639029</td>\n",
       "      <td>1.38781</td>\n",
       "      <td>0.681051</td>\n",
       "      <td>0.950629</td>\n",
       "      <td>0.460185</td>\n",
       "      <td>0.496246</td>\n",
       "      <td>0.493607</td>\n",
       "      <td>0.970607</td>\n",
       "      <td>0.449299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.43091</td>\n",
       "      <td>0.479272</td>\n",
       "      <td>1.73477</td>\n",
       "      <td>0.510788</td>\n",
       "      <td>0.950629</td>\n",
       "      <td>0.690278</td>\n",
       "      <td>0.496246</td>\n",
       "      <td>0.370205</td>\n",
       "      <td>0.485303</td>\n",
       "      <td>0.224649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.43091</td>\n",
       "      <td>0.319515</td>\n",
       "      <td>1.38781</td>\n",
       "      <td>0.681051</td>\n",
       "      <td>1.18829</td>\n",
       "      <td>0.230093</td>\n",
       "      <td>0.620308</td>\n",
       "      <td>0.493607</td>\n",
       "      <td>0.485303</td>\n",
       "      <td>0.22465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.323182</td>\n",
       "      <td>0.798787</td>\n",
       "      <td>1.73477</td>\n",
       "      <td>0.340525</td>\n",
       "      <td>1.18829</td>\n",
       "      <td>0.345139</td>\n",
       "      <td>0.620308</td>\n",
       "      <td>0.370205</td>\n",
       "      <td>0.970607</td>\n",
       "      <td>0.561624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.646365</td>\n",
       "      <td>0.479272</td>\n",
       "      <td>1.04086</td>\n",
       "      <td>1.02158</td>\n",
       "      <td>1.18829</td>\n",
       "      <td>0.575232</td>\n",
       "      <td>0.248123</td>\n",
       "      <td>0.617009</td>\n",
       "      <td>1.21326</td>\n",
       "      <td>0.336975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.215455</td>\n",
       "      <td>0.639029</td>\n",
       "      <td>1.38781</td>\n",
       "      <td>0.340525</td>\n",
       "      <td>0.475314</td>\n",
       "      <td>0.575232</td>\n",
       "      <td>0.744369</td>\n",
       "      <td>0.246803</td>\n",
       "      <td>0.970607</td>\n",
       "      <td>0.4493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.646365</td>\n",
       "      <td>0.319515</td>\n",
       "      <td>1.38781</td>\n",
       "      <td>0.510788</td>\n",
       "      <td>0.950629</td>\n",
       "      <td>0.690278</td>\n",
       "      <td>0.496246</td>\n",
       "      <td>0.246804</td>\n",
       "      <td>1.21326</td>\n",
       "      <td>0.561624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.646365</td>\n",
       "      <td>0.639029</td>\n",
       "      <td>1.38781</td>\n",
       "      <td>0.340525</td>\n",
       "      <td>0.475314</td>\n",
       "      <td>0.230093</td>\n",
       "      <td>0.744369</td>\n",
       "      <td>0.370205</td>\n",
       "      <td>1.21326</td>\n",
       "      <td>0.22465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.215455</td>\n",
       "      <td>0.958544</td>\n",
       "      <td>1.73477</td>\n",
       "      <td>0.340525</td>\n",
       "      <td>0.712971</td>\n",
       "      <td>0.345139</td>\n",
       "      <td>0.620308</td>\n",
       "      <td>0.246803</td>\n",
       "      <td>0.970607</td>\n",
       "      <td>0.561624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.323182</td>\n",
       "      <td>0.798787</td>\n",
       "      <td>1.73477</td>\n",
       "      <td>1.02158</td>\n",
       "      <td>0.475314</td>\n",
       "      <td>0.690278</td>\n",
       "      <td>0.248123</td>\n",
       "      <td>0.493607</td>\n",
       "      <td>1.21326</td>\n",
       "      <td>0.22465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          im1       im2      im3       im4       pm1       pm2       pm3  \\\n",
       "0     0.43091  0.639029  1.38781  0.681051  0.950629  0.460185  0.496246   \n",
       "1     0.43091  0.479272  1.73477  0.510788  0.950629  0.690278  0.496246   \n",
       "2     0.43091  0.319515  1.38781  0.681051   1.18829  0.230093  0.620308   \n",
       "3    0.323182  0.798787  1.73477  0.340525   1.18829  0.345139  0.620308   \n",
       "4    0.646365  0.479272  1.04086   1.02158   1.18829  0.575232  0.248123   \n",
       "..        ...       ...      ...       ...       ...       ...       ...   \n",
       "96   0.215455  0.639029  1.38781  0.340525  0.475314  0.575232  0.744369   \n",
       "97   0.646365  0.319515  1.38781  0.510788  0.950629  0.690278  0.496246   \n",
       "98   0.646365  0.639029  1.38781  0.340525  0.475314  0.230093  0.744369   \n",
       "99   0.215455  0.958544  1.73477  0.340525  0.712971  0.345139  0.620308   \n",
       "100  0.323182  0.798787  1.73477   1.02158  0.475314  0.690278  0.248123   \n",
       "\n",
       "          pm4       pm5       pm6  \n",
       "0    0.493607  0.970607  0.449299  \n",
       "1    0.370205  0.485303  0.224649  \n",
       "2    0.493607  0.485303   0.22465  \n",
       "3    0.370205  0.970607  0.561624  \n",
       "4    0.617009   1.21326  0.336975  \n",
       "..        ...       ...       ...  \n",
       "96   0.246803  0.970607    0.4493  \n",
       "97   0.246804   1.21326  0.561624  \n",
       "98   0.370205   1.21326   0.22465  \n",
       "99   0.246803  0.970607  0.561624  \n",
       "100  0.493607   1.21326   0.22465  \n",
       "\n",
       "[101 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree: 4\n",
      "n_epochs: 10\n",
      "Quantity: pp1\n",
      "nn: [12, 6]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 19:39:46.864641: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2021-12-23 19:39:46.954138: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300000000 Hz\n",
      "2021-12-23 19:39:46.955193: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556507bfe370 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-12-23 19:39:46.955830: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2021-12-23 19:39:46.956108: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Updated file: ../results/error_summaries/batch-OSR-CV-quick.csv (curr. length: 1)\n",
      "degree: 5\n",
      "n_epochs: 10\n",
      "Quantity: pp1\n",
      "nn: [12, 6]\n",
      "\n",
      "Updated file: ../results/error_summaries/batch-OSR-CV-quick.csv (curr. length: 2)\n",
      "degree: 6\n",
      "n_epochs: 10\n",
      "Quantity: pp1\n",
      "nn: [12, 6]\n",
      "\n",
      "Updated file: ../results/error_summaries/batch-OSR-CV-quick.csv (curr. length: 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>train_idx</th>\n",
       "      <th>quantity</th>\n",
       "      <th>degree</th>\n",
       "      <th>nn</th>\n",
       "      <th>n_epochs</th>\n",
       "      <th>loss</th>\n",
       "      <th>k</th>\n",
       "      <th>mape_list</th>\n",
       "      <th>mae_list</th>\n",
       "      <th>mape_avg</th>\n",
       "      <th>mae_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-23 19:41:06.860257</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>pp1</td>\n",
       "      <td>4</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>10</td>\n",
       "      <td>mae</td>\n",
       "      <td>10</td>\n",
       "      <td>[14.331176846576543, 3.40770548520133, 2.36013...</td>\n",
       "      <td>[2649.789763556826, 674.6109988724106, 553.434...</td>\n",
       "      <td>8.977256</td>\n",
       "      <td>1856.447272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-23 19:42:35.050184</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>pp1</td>\n",
       "      <td>5</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>10</td>\n",
       "      <td>mae</td>\n",
       "      <td>10</td>\n",
       "      <td>[27.456637417699564, 14.983467066400676, 13.11...</td>\n",
       "      <td>[5044.152524204159, 2943.330869447975, 3098.16...</td>\n",
       "      <td>10.350087</td>\n",
       "      <td>2147.584989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-12-23 19:43:52.975104</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>pp1</td>\n",
       "      <td>6</td>\n",
       "      <td>[12, 6]</td>\n",
       "      <td>10</td>\n",
       "      <td>mae</td>\n",
       "      <td>10</td>\n",
       "      <td>[13.573262544441574, 12.339955892277068, 3.660...</td>\n",
       "      <td>[2514.630590167876, 2430.177386765585, 862.611...</td>\n",
       "      <td>8.941384</td>\n",
       "      <td>1850.860536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp  \\\n",
       "0 2021-12-23 19:41:06.860257   \n",
       "0 2021-12-23 19:42:35.050184   \n",
       "0 2021-12-23 19:43:52.975104   \n",
       "\n",
       "                                           train_idx quantity degree       nn  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      pp1      4  [12, 6]   \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      pp1      5  [12, 6]   \n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...      pp1      6  [12, 6]   \n",
       "\n",
       "  n_epochs loss   k                                          mape_list  \\\n",
       "0       10  mae  10  [14.331176846576543, 3.40770548520133, 2.36013...   \n",
       "0       10  mae  10  [27.456637417699564, 14.983467066400676, 13.11...   \n",
       "0       10  mae  10  [13.573262544441574, 12.339955892277068, 3.660...   \n",
       "\n",
       "                                            mae_list   mape_avg      mae_avg  \n",
       "0  [2649.789763556826, 674.6109988724106, 553.434...   8.977256  1856.447272  \n",
       "0  [5044.152524204159, 2943.330869447975, 3098.16...  10.350087  2147.584989  \n",
       "0  [2514.630590167876, 2430.177386765585, 862.611...   8.941384  1850.860536  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# This cell: \n",
    "# 1) sets aside 10% of data for TESTING, and 2) performs k-fold cross-validation on the training data (90% of all data)\n",
    "\n",
    "# Like mentioned at the top of the notebook, the code here is written to run \"incrementally\": \n",
    "# it will find saved results (in a file like: ../results/error_summaries/batch-OSR-CV-test.csv) if there are any,\n",
    "# and continue running analysis for missing configurations.\n",
    "\n",
    "##########################\n",
    "# Setting key parameters #\n",
    "##########################\n",
    "\n",
    "batch_id = \"OSR-CV-quick\"\n",
    "\n",
    "loss = \"mae\" # mae or mse\n",
    "validation_split_ratio = 0.0 # Ratio of train set that will be treated as validation set\n",
    "\n",
    "k = 10 # k for k-fold validation\n",
    "\n",
    "plotting = False\n",
    "\n",
    "res_dir = \"../results/error_summaries/\"\n",
    "model_dir = \"../models/\"\n",
    "\n",
    "save_to_disk = False\n",
    "\n",
    "quick_test = True\n",
    "\n",
    "if quick_test:\n",
    "    quantity_list = [\"pp1\"]\n",
    "    degree_list = [4,5,6]\n",
    "    nn_list = [[12, 6]]\n",
    "    n_epochs_list = [10]   \n",
    "else:\n",
    "    # This configuration will require a lot of compute time!\n",
    "    quantity_list = [\"pp1\", \"pp2\", \"pp3\", \"pp4\", \"pp5\", \"pp6\", \"pt1\", \"pt2\", \"pt3\", \"pt4\", \"pt5\", \"pt6\"]\n",
    "    degree_list = [4,5,6]\n",
    "    nn_list = [[12, 6],\n",
    "               [12, 12],\n",
    "               [16, 8],\n",
    "               [16, 16],\n",
    "               [24, 12],\n",
    "               [24, 24],\n",
    "               [32, 16],\n",
    "               [32, 32]]\n",
    "    n_epochs_list = [250, 500, 1000]   \n",
    "\n",
    "# New results will be *appended* to this file\n",
    "dest_file = os.path.join(res_dir, \"batch-%s.csv\" % (str(batch_id)))\n",
    "\n",
    "required_results_df_columns = [\"timestamp\", \"train_idx\", \"quantity\", \"degree\", \"nn\", \n",
    "                               \"n_epochs\", \"loss\", \"k\", \"mape_list\", \"mae_list\", \"mape_avg\", \"mae_avg\"]                        \n",
    "if os.path.exists(dest_file):\n",
    "    existing_results_df = pd.read_csv(dest_file)\n",
    "else:\n",
    "    # Empty dataframe with required columns\n",
    "    existing_results_df = pd.DataFrame(columns = required_results_df_columns)\n",
    "    \n",
    "##################################\n",
    "# All key parameters are set now #\n",
    "##################################\n",
    "\n",
    "##################################\n",
    "# Routine for model fitting      #\n",
    "##################################\n",
    "\n",
    "def fit_custom_model(X_train, Y_train, nn, loss, n_epochs):\n",
    "    \n",
    "    assert len(nn) == 2, \"nn: should be a list with *two* layer sizes\"\n",
    "    nn1, nn2 = nn\n",
    "    \n",
    "    # initialize model\n",
    "    model = Sequential()\n",
    "\n",
    "    # add 1st layer\n",
    "    model.add(Dense(\n",
    "        units=nn1,\n",
    "        input_dim=X_train.shape[1],\n",
    "        kernel_initializer='glorot_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        activation='tanh') \n",
    "    )\n",
    "    # add 2nd layer\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=nn2,\n",
    "            input_dim=nn1,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='zeros',\n",
    "            activation='tanh')\n",
    "        )\n",
    "    # add output layer\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=Y_train.shape[1],\n",
    "            input_dim=nn2,\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            bias_initializer='zeros',\n",
    "            activation=None)\n",
    "        )\n",
    "\n",
    "    # define SGD optimizer\n",
    "    sgd_optimizer = SGD(\n",
    "        lr=0.001, decay=1e-7, momentum=0.9\n",
    "    )\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(\n",
    "        optimizer=sgd_optimizer,\n",
    "        loss=loss\n",
    "    )\n",
    "\n",
    "    X_train_converted=np.asarray(X_train).astype(np.float32)\n",
    "    Y_train_converted=np.asarray(Y_train).astype(np.float32)\n",
    "\n",
    "    #print(model.summary())\n",
    "\n",
    "    #train model; validation_split=0.0 because CV is already handled outside of this training routine\n",
    "    training_history = model.fit(\n",
    "        X_train_converted, Y_train_converted,\n",
    "        batch_size=1, \n",
    "        epochs=n_epochs,\n",
    "        verbose=0, \n",
    "        validation_split=0.0\n",
    "    )\n",
    "    return model, training_history\n",
    "\n",
    "#################################### \n",
    "# End of Routine for model fitting #\n",
    "####################################\n",
    "\n",
    "# Set aside indices for TESTING (after cross validation)\n",
    "all_idx = np.array(range(ens.count))\n",
    "kfold = KFold(10, True, 1) # 10-fold results in 10% of all indices (101) being exactly 11 cases\n",
    "train, test = next(kfold.split(all_idx))\n",
    "train_idx, test_idx = all_idx[train], all_idx[test]\n",
    "print('train: %s, test: %s' % (train_idx, test_idx))\n",
    "\n",
    "# Form a dataframe with all controlled parameter values\n",
    "m_column_list = [\"im1\", \"im2\", \"im3\", \"im4\"]\n",
    "injector_df = pd.DataFrame(index=range(ens.count), columns=m_column_list)\n",
    "for i in range(ens.count):\n",
    "    df = ens[i].scaled_timeseries[m_column_list]\n",
    "    df = df[df.index >= pd.to_datetime('2020-01-01')]\n",
    "\n",
    "    for c in m_column_list:\n",
    "        # Important: this characterizes a timeseries by its most common values; different methods can be used here\n",
    "        # This works well for the step function\n",
    "        injector_df.at[i, c] = df[c].value_counts().index[0]\n",
    "\n",
    "m_column_list = [\"pm1\", \"pm2\", \"pm3\", \"pm4\", \"pm5\", \"pm6\"]\n",
    "producer_df = pd.DataFrame(index=range(ens.count), columns=m_column_list)\n",
    "for i in range(ens.count):\n",
    "    df = ens[i].scaled_timeseries[m_column_list]\n",
    "    df = df[df.index >= pd.to_datetime('2020-01-01')]\n",
    "\n",
    "    for c in m_column_list:\n",
    "        # Important: this characterizes a timeseries by its most common values; different methods can be used here\n",
    "        # This works well for the step function\n",
    "        producer_df.at[i, c] = df[c].value_counts().index[0]\n",
    "\n",
    "all_wells_df = injector_df.join(producer_df)\n",
    "display(all_wells_df)\n",
    "\n",
    "for degree in degree_list:\n",
    "    print(\"degree:\", degree)\n",
    "    poly = get_polynomial_func(degree=degree)\n",
    "    \n",
    "    for n_epochs in n_epochs_list: \n",
    "        print(\"n_epochs:\", n_epochs)\n",
    "        for quantity in quantity_list:\n",
    "            print(\"Quantity:\", quantity)\n",
    "            for nn in nn_list:\n",
    "                print(\"nn:\", nn)\n",
    "\n",
    "                if len(existing_results_df) > 0:\n",
    "                    matching_existing_results = existing_results_df[(existing_results_df[\"degree\"] == degree) &\n",
    "                                                                    (existing_results_df[\"n_epochs\"] == n_epochs) &\n",
    "                                                                    (existing_results_df[\"quantity\"] == quantity) &\n",
    "                                                                    (existing_results_df[\"nn\"] == str(nn))]\n",
    "                    if len(matching_existing_results) > 0:\n",
    "                        print(\"\\nSkipping (found existing results).\")\n",
    "                        continue\n",
    "                         \n",
    "                if quantity in [\"pp1\", \"pp2\", \"pp3\", \"pp4\", \"pp5\", \"pp6\"]:\n",
    "                    scaler = ens.common_pres_scaler\n",
    "                elif quantity in [\"pt1\", \"pt2\", \"pt3\", \"pt4\", \"pt5\", \"pt6\"]:\n",
    "                    scaler = ens.common_temp_scaler\n",
    "\n",
    "                t_mapper = ens.shared_scaled_time_index(start_at='2020-01-01')\n",
    "                #t_mapper = {k: v+1.0 for k,v in t_mapper.items()}\n",
    "\n",
    "                r2_vector = []\n",
    "                rmse_vector = []\n",
    "\n",
    "                ydata_df = pd.DataFrame(index=sorted(t_mapper.values()), columns=range(ens.count))\n",
    "                yhat_df = pd.DataFrame(index=sorted(t_mapper.values()), columns=range(ens.count))\n",
    "\n",
    "                coeff_df, _, _ = ens.get_curve_approximations(quantity, poly)\n",
    "\n",
    "                #display(coeff_df)\n",
    "\n",
    "                mape_list, mae_list = [], []\n",
    "\n",
    "                # Further split train set into actual train and validate subset within k-fold validation routine\n",
    "                kfold_within_train = KFold(k, True, 1) # k-fold CV\n",
    "                for kfold_id, (actual_train, actual_val) in enumerate(kfold_within_train.split(train_idx)):\n",
    "                    actual_train_idx, actual_val_idx = train_idx[actual_train], train_idx[actual_val]\n",
    "                    #print('actual train: %s, actual val: %s' % (actual_train_idx, actual_val_idx))\n",
    "\n",
    "                    #print(\"Input for Curve ML training:\")\n",
    "                    X_train = all_wells_df.loc[actual_train_idx]\n",
    "                    #display(X_train)\n",
    "\n",
    "                    #print(\"Output for Curve ML training:\")\n",
    "                    Y_train = coeff_df.loc[actual_train_idx]\n",
    "                    #display(Y_train)\n",
    "\n",
    "                    X_val = all_wells_df.loc[actual_val_idx]\n",
    "                    Y_val = coeff_df.loc[actual_val_idx]\n",
    "\n",
    "                    model, training_history = fit_custom_model(X_train, Y_train, nn, loss, n_epochs)\n",
    "\n",
    "                    # Random UUID\n",
    "                    model_uuid = uuid.uuid4()\n",
    "                    if save_to_disk:\n",
    "                        model.save(os.path.join(model_dir, str(model_uuid)))\n",
    "\n",
    "                    X_val=np.asarray(X_val).astype(np.float32)\n",
    "                    Y_val=np.asarray(Y_val).astype(np.float32)\n",
    "\n",
    "                    # Predicted coefficients\n",
    "                    Y_val_pred = model.predict(X_val, verbose=0)\n",
    "\n",
    "                    xdata = np.linspace(0, 1.0, 50) # point along scaled time dimension\n",
    "\n",
    "                    for coeff_true, coeff_pred, val_case_id in zip(Y_val, Y_val_pred, actual_val_idx):\n",
    "                        one_traj = ens[val_case_id].scaled_timeseries[quantity]\n",
    "                        one_traj = one_traj[one_traj.index >= pd.to_datetime('2020-01-01')]\n",
    "                        xdata_for_true_data = np.array(one_traj.index.map(t_mapper))\n",
    "                        ydata_not_fitted = one_traj.values\n",
    "\n",
    "                        ydata_true = poly(xdata, *coeff_true)\n",
    "                        ydata_pred = poly(xdata, *coeff_pred)\n",
    "                        ydata_pred_for_error_est = poly(xdata_for_true_data, *coeff_pred)\n",
    "\n",
    "                        # Perform error analysis on unscaled data\n",
    "                        ydata_not_fitted_unscaled = scaler.inverse_transform(ydata_not_fitted.reshape(-1, 1)).reshape(-1,)\n",
    "                        ydata_pred_for_error_est_unscaled = \\\n",
    "                            scaler.inverse_transform(ydata_pred_for_error_est.reshape(-1, 1)).reshape(-1,)\n",
    "                        mape = mean_absolute_percentage_error(ydata_not_fitted_unscaled, ydata_pred_for_error_est_unscaled)\n",
    "                        mae = mean_absolute_error(ydata_not_fitted_unscaled, ydata_pred_for_error_est_unscaled)\n",
    "\n",
    "                        mape_list.append(mape)\n",
    "                        mae_list.append(mae)\n",
    "\n",
    "                now = datetime.now()\n",
    "                new_results_df = pd.DataFrame(columns = required_results_df_columns)\n",
    "                new_results_df.loc[len(new_results_df)] = [\n",
    "                    now, \n",
    "                    train_idx,\n",
    "                    quantity, degree, nn, n_epochs, loss, \n",
    "                    k,\n",
    "                    mape_list, mae_list, np.array(mape_list).mean(), np.array(mae_list).mean()]\n",
    "                \n",
    "                # Combine existing and new results\n",
    "                if len(existing_results_df) > 0:\n",
    "                    existing_results_df = pd.concat([existing_results_df, new_results_df])\n",
    "                else: \n",
    "                    existing_results_df = new_results_df\n",
    "                \n",
    "                existing_results_df.to_csv(dest_file, index=False)\n",
    "                print(\"\\nUpdated file: %s (curr. length: %d)\" % (dest_file, len(existing_results_df)))\n",
    "                \n",
    "display(existing_results_df)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
